function ys=mylowess(xy,xs,span)

%MYLOWESS Lowess smoothing, preserving x values
% YS=MYLOWESS(XY,XS) returns the smoothed version of the x/y data in the
% two-column matrix XY, but evaluates the smooth at XS and returns the
% smoothed values in YS. Any values outside the range of XY are taken to
% be equal to the closest values.

if nargin<3 || isempty(span)
    span = .3;
end

% Sort and get smoothed version of xy data
xy = sortrows(xy);
x1 = xy(:,1);
y1 = xy(:,2);
ys1 = smooth(x1,y1,span,'loess');

% Remove repeats so we can interpolate
t = diff(x1)==0;
x1(t)=[]; ys1(t) = [];

% Interpolate to evaluate this at the xs values
ys = interp1(x1,ys1,xs,'linear',NaN);

% Some of the original points may have x values outside the range of the
% resampled data. Those are now NaN because we could not interpolate them.
% Replace NaN by the closest smoothed value. This amounts to extending the
% smooth curve using a horizontal line.
if any(isnan(ys))
    ys(xs<x1(1)) = ys1(1);
    ys(xs>x1(end)) = ys1(end);
end

%______________________________________________________________________
%Demo Code
%_______________________________________________________________________


%% Framing the problem, Part 1.


% The following video is an introduction to nonparameteric fitting using

% Statistics Toolbox and Curve Fitting Toolbox.


% Let's assume that you need to model a relationship between two variables:

% X and Y. The "true" relationship is described by this Blue Curve.


hold off

X = linspace(1,10,100);

X = X';


w = randn;

a0 = randn;

a1 = randn;

b1 = randn;

a2 = randn;

b2 = randn;


Y = a0 + a1*cos(X*w) + b1*sin(X*w) + a2*cos(2*X*w) + b2*sin(2*X*w);


actual = plot(X,Y);



%% Framing the problem, part 2


% Unfortunately, you don't actually know what this curve looks like.

% The only thing that you have to work with is some noisy samples drawn

% from the curve.


delete (actual)


K = max(Y) - min(Y);


noisy = Y + .2*K*randn(100,1);


hold on

sample = scatter(X,noisy,'b');


% Your task is to do as good a job as possible estimating the curve

% given your noisy sample.


%% Parametric fitting


% In many cases, you can apply domain knowledge to make your task easier.

% For example, suppose that you were modeling the lift generated by a wing.

% We know, from first principles, that this can be modeled using

% Bernoulli's equation.


% In this artificial example, because I know that the blue curve was

% generated using a second order Fourier series, I can apply

% nonlinear regression like so.


% <Move to cell model. Execute the following cell>


f = fit(X,noisy,'fourier2');

fourier = plot(f, 'r');

actual = plot(X,Y);


% As you can see, our domain knowledge allows us to do a great job

% estimating the true relationship from the noisy data sample.


%% Now life gets complicated


% Unfortunately, there are any number of cases where we dont have any

% domain knowledge that we can fall back on.


delete (actual)

delete(fourier)

delete(legend)


% Here, the only thing that we have available is our noisy data set.


% Luckily, we can apply some advanced statistical techniques like localized

% regression and cross validation and still generate a quite reasonable

% estimation.


%% Show localized regression.


% Localized regression is a smoothing technique. We fit a large number of

% curves to local subsets of the data and then combine these into a single

% curve.


% The accuracy of your localized regression model depends on choosing the

% right smoothing parameter. In this example, we're using brute force to

% test 490 different possible smoothing parameters. We're applying a

% technique called cross validation to select the most accurate smoothing

% parameter.


num = 490;

spans = linspace(.01,.99,num);

sse = zeros(size(spans));

cp = cvpartition(100,'k',10);


for j=1:length(spans)

f = @(train,test) norm(test(:,2) - mylowess(train,test(:,1),spans(j)))^2;

sse(j) = sum(crossval(f,[X,noisy],'partition',cp));

end


[minsse,minj] = min(sse);

span = spans(minj);


x = linspace(min(X),max(X));

loess = line(x,mylowess([X,noisy],x,span),'color','k','linestyle','-',
'linewidth',2)


% As you can see, our localized regression model has generated a very

% accurate estimate of our curve without any requirement that we specify a

% relationship between X and Y.


%% Visually compare our models


% Here you can compare the relative accuracy of the two regression models

% estimating the "true" relationship between X and Y


delete(sample);

f = fit(X,noisy,'fourier2');

fourier = plot(f, 'r');

delete(legend);

actual = plot(X,Y);


%% Confidence Intervals


delete(loess)

delete(fourier)

delete(actual)


% In many cases, you aren't just interested in estimating the relationship

% between X and Y. You also want to generate some confidence intervals

% that you can use to show how certain you are about the accuracy of this

% curve.


f = @(xy) mylowess(xy,X,span);

yboot2 = bootstrp(1000,f,[X,noisy])';

% yboot2 = bootstrp(1000,f,[X,noisy])';

meanloess = mean(yboot2,2);

h1 = line(X, meanloess,'color','k','linestyle','-','linewidth',2);


% Here we're using a related technique called the bootstrap to generate

% confidence intervals for X and Y.


stdloess = std(yboot2,0,2);

h2 = line(X,
meanloess+2*stdloess,'color','r','linestyle','--','linewidth',2);

h3 = line(X,
meanloess-2*stdloess,'color','r','linestyle','--','linewidth',2);